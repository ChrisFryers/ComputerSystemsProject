# import pandas_datareader as webreader # Remote data access for pandas
import math # Mathematical functions 
import numpy as np # Fundamental package for scientific computing with Python
import pandas as pd # Additional functions for analysing and manipulating data
from datetime import date, timedelta, datetime # Date Functions
from pandas.plotting import register_matplotlib_converters # This function adds plotting functions for calender dates
import matplotlib.pyplot as plt # Important package for visualization - we use this to plot the market data
import matplotlib.dates as mdates # Formatting dates
from sklearn.metrics import mean_absolute_error, mean_squared_error # Packages for measuring model performance / errors
from keras.models import Sequential # Deep learning library, used for neural networks
from keras.layers import LSTM, Dense, Dropout # Deep learning classes for recurrent and regular densely-connected layers
from keras.callbacks import EarlyStopping # EarlyStopping during model training
from sklearn.preprocessing import RobustScaler, MinMaxScaler # This Scaler removes the median and scales the data according to the quantile range to normalize the price data 
import seaborn as sns
import datetime as dt




# get data
df = pd.read_csv(r'C:\Users\Chris\Desktop\UlsterUniversity\Final Project\PySimpleGUI\Closedf.csv', parse_dates = True, index_col = 'Date')
df = df[['EURUSD=X', 'GBPUSD=X']]

# specifying period
end = dt.datetime.now()  # end date is now
start = dt.datetime(2005, 1, 1)   # start date  
df = df[start:end]
# setting frequency
df = df.asfreq('D') # changes frequency to daily
#df.index # checks the frequency at bottom of printout
    
# fills NaN's
df = df.fillna(method = 'backfill')  # takes the close rows and fills in missing values back
df = df.fillna(method = 'ffill')  # takes the close rows and fills in missing values forwards, for some markets operating at different days
    



# creating dataframe for predictions

# creating the date range
forecast_start = dt.datetime.now()  # end date is now
#print(forecast_start)
forecast_end = dt.datetime.now() + timedelta(days = 9)  # starting day + forecast days - 1. 
#print(forecast_end)
# create date range
dateRange = pd.date_range(start = forecast_start, end = forecast_end, freq = 'D')
# creating empty dataframe
df_forecast = pd.DataFrame(index = dateRange)




# create variable for number of columns Num_of_cols = len(list(df))


## start loop to loop around column names and make predictions for everyone
for column_name in df:

    #assetName = df[column_name]

    # Indexing Batches
    df_train = df.sort_values(by=['Date']).copy()

    # Save a copy of the dates index, before we need to reset it to numbers
    date_index = df_train.index

    # We reset the index, so we can convert the date-index to a number-index
    df_train = df_train.reset_index(drop=True).copy()






    # preparing the data w/ feature engineering

    def prepare_data(df):

        # List of considered Features - change to list of headings in df (df.index())
        # using all columns in df

        # Create the dataset with features and filter the data to the list of FEATURES
        df_filter = df # using all columns in df
    
        # Convert the data to numpy values
        np_filter_unscaled = np.array(df_filter)
        #np_filter_unscaled = np.reshape(np_unscaled, (df_filter.shape[0], -1))
        #print(np_filter_unscaled.shape)

        np_c_unscaled = np.array(df).reshape(-1, 1)
    
        return np_filter_unscaled, np_c_unscaled
    
    np_filter_unscaled, np_c_unscaled = prepare_data(df_train)
                                          
    # Creating a separate scaler that works on a single column for scaling predictions
    # Scale each feature to a range between 0 and 1
    scaler_train = MinMaxScaler()
    np_scaled = scaler_train.fit_transform(np_filter_unscaled)
    
    # Create a separate scaler for a single column
    scaler_pred = MinMaxScaler()
    np_scaled_c = scaler_pred.fit_transform(np_c_unscaled)  






    # Set the input_sequence_length length - this is the timeframe used to make a single prediction - experiment witht his number to find optimal accuracy
    input_sequence_length = 50
    # The output sequence length is the number of steps that the neural network predicts
    output_sequence_length = 10 #

    # Prediction Index -  *** will need to change this to a variable to loop around df
    index_Close = df_train.columns.get_loc(column_name)

    # Split the training data into train and train data sets
    # As a first step, we get the number of rows to train the model on 80% of the data 
    train_data_length = math.ceil(np_scaled.shape[0] * 0.8)

    # Create the training and test data
    train_data = np_scaled[0:train_data_length, :]
    test_data = np_scaled[train_data_length - input_sequence_length:, :]






    # The RNN needs data with the format of [samples, time steps, features]
    # Here, we create N samples, input_sequence_length time steps per sample, and f features
    def partition_dataset(input_sequence_length, output_sequence_length, data):
        x, y = [], []
        data_len = data.shape[0]
        for i in range(input_sequence_length, data_len - output_sequence_length):
            x.append(data[i-input_sequence_length:i,:]) #contains input_sequence_length values 0-input_sequence_length * columns
            y.append(data[i:i + output_sequence_length, index_Close]) #contains the prediction values for validation (3rd column = Close),  for single-step prediction
    
        # Convert the x and y to numpy arrays
        x = np.array(x)
        y = np.array(y)
        return x, y

    # Generate training data and test data
    x_train, y_train = partition_dataset(input_sequence_length, output_sequence_length, train_data)
    x_test, y_test = partition_dataset(input_sequence_length, output_sequence_length, test_data)


    # Print the shapes: the result is: (rows, training_sequence, features) (prediction value, )
    #print(x_train.shape, y_train.shape)
    #print(x_test.shape, y_test.shape)







    # Validate that the prediction value and the input match up
    # The last close price of the second input sample should equal the first prediction value
    # print(x_train[1][input_sequence_length-1][index_Close])
    # print(y_train[0])








    # prepare the neural network archiecture and train th emulti-output regression model

    # Configure the neural network model
    model = Sequential()
    n_output_neurons = output_sequence_length

    # Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables
    n_input_neurons = x_train.shape[1] * x_train.shape[2]
    print(n_input_neurons, x_train.shape[1], x_train.shape[2])
    model.add(LSTM(n_input_neurons, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2]))) 
    model.add(LSTM(n_input_neurons, return_sequences=False))
    model.add(Dense(20))
    model.add(Dense(n_output_neurons))

    # Compile the model
    model.compile(optimizer='adam', loss='mse')







    # Training the model
    epochs = 3 # used to be 10
    batch_size = 16
    early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)
    history = model.fit(x_train, y_train, 
                        batch_size=batch_size, 
                        epochs=epochs,
                        validation_data=(x_test, y_test)
                        )
                    
                        #callbacks=[early_stop])










    # Get the predicted values
    y_pred_scaled = model.predict(x_test)

    # Unscale the predicted values
    y_pred = scaler_pred.inverse_transform(y_pred_scaled)
    y_test_unscaled = scaler_pred.inverse_transform(y_test).reshape(-1, output_sequence_length)
    y_test_unscaled.shape

    # Mean Absolute Error (MAE)
    MAE = mean_absolute_error(y_test_unscaled, y_pred)
    #print(f'Median Absolute Error (MAE): {np.round(MAE, 2)}')

    # Mean Absolute Percentage Error (MAPE)
    MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100
    #print(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %')

    # Median Absolute Percentage Error (MDAPE)
    MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100
    #print(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')











    x_test_unscaled = scaler_pred.inverse_transform(np.array(pd.DataFrame(x_test[0])[index_Close]).reshape(-1, 1)) 
    df_test = pd.DataFrame(x_test_unscaled)










    # get the highest index from the x_test dataset
    index_max = x_test.shape[0]
    x_test_new = np_scaled[-51:-1,:].reshape(1,50,len(list(df))) # changed last digit from 2 to number of columns in dataframe

    # undo the scaling of the predictions
    y_pred_scaled = model.predict(x_test_new)
    y_pred = scaler_pred.inverse_transform(y_pred_scaled)

    # plot the predictions
    #plot_new_multi_forecast(0, 0, x_test_new, y_pred)


    # code to put predictions in dataframe
    y_pred = np.array(y_pred)

    y_pred_l = np.ndarray.tolist(y_pred[0])

    #print(type(y_pred_l))
    #print(y_pred_l)



    # adding the forecast to the forecast dataframe
    df_forecast[column_name] = y_pred_l
    #plt.show()
    #print(df_forecast)

    plt.plot(df_forecast[column_name])
    plt.title(column_name)
    plt.show()
    

